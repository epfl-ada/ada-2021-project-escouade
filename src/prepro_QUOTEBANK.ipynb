{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6efbeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-12T17:48:01.165073Z",
     "iopub.status.busy": "2021-12-12T17:48:01.164567Z",
     "iopub.status.idle": "2021-12-12T17:48:03.592627Z",
     "shell.execute_reply": "2021-12-12T17:48:03.591837Z",
     "shell.execute_reply.started": "2021-12-12T17:48:01.165032Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from ressources import config\n",
    "from ressources.empath_cat import Empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9483e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAW_DATA_FOLDER = config.RAW_DATA_FOLDER\n",
    "GENERATED_DATA_FOLDER = config.GENERATED_DATA_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4efe66",
   "metadata": {},
   "source": [
    "## QUOTEBANK Dataset\n",
    "Exploring all Quotebank datasets (2015 to 2019) to have an idea of their size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "315d32bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "QUOTEBANK_FOLDER = RAW_DATA_FOLDER / \"QUOTEBANK\"\n",
    "file_list = list(QUOTEBANK_FOLDER.glob('*.json.bz2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7043493",
   "metadata": {},
   "source": [
    "### List of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a204c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pd.read_json(file_list[0], lines=True, compression='bz2', chunksize=1) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        df_quotebank = chunk\n",
    "        break\n",
    "# column list for Quotebank dataset\n",
    "print(f\"\\nColumns quotebank:\\n{df_quotebank.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ab53ae",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d24d8bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample for Quotebank dataset\n",
    "print(\"\\nSample quotebank:\\n\")\n",
    "display(df_quotebank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b15a9",
   "metadata": {},
   "source": [
    "### Number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a505f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of rows for quotebank dataset\n",
    "chunksize = 10000\n",
    "for file in file_list:\n",
    "    n = 0\n",
    "    print(f\"Processing file {file.name}\")\n",
    "    with pd.read_json(file, lines=True, compression='bz2', chunksize=chunksize) as df_reader:\n",
    "        for chunk in df_reader:\n",
    "            n += len(chunk)\n",
    "            print(n, end = \"\\r\")\n",
    "    \n",
    "    print(f\"{n} rows in {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d40ca3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-10T09:42:24.070186Z",
     "iopub.status.busy": "2021-11-10T09:42:24.067422Z",
     "iopub.status.idle": "2021-11-10T10:39:35.926430Z",
     "shell.execute_reply": "2021-11-10T10:39:35.925677Z",
     "shell.execute_reply.started": "2021-11-10T09:42:24.070126Z"
    },
    "tags": []
   },
   "source": [
    "Output:\n",
    "\n",
    "Processing file quotes-2015.json.bz2</br>\n",
    "20874338 rows in quotes-2015.json.bz2</br>\n",
    "Processing file quotes-2016.json.bz2</br>\n",
    "13862129 rows in quotes-2016.json.bz2</br>\n",
    "Processing file quotes-2017.json.bz2</br>\n",
    "26611588 rows in quotes-2017.json.bz2</br>\n",
    "Processing file quotes-2018.json.bz2</br>\n",
    "27228451 rows in quotes-2018.json.bz2</br>\n",
    "Processing file quotes-2019.json.bz2</br>\n",
    "21763302 rows in quotes-2019.json.bz2</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885fdd93",
   "metadata": {},
   "source": [
    "## Open quotebank 2018 and add columns for gender and dob \n",
    "In concert with the assistant, we decide to focalise on only one year. We choose 2018 because it's the year with the most quotes. Here we use the wikidata dataframe we created in the prepro_WIKIDATA.ipynb to add gender and date of birth for each quotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69b48f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open wikidata dictionary\n",
    "with open('../generated/WIKIDATA/dict_wikidata_people.pickle', 'rb') as f: \n",
    "     dict_wikidata_people = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18aab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = '../data/QUOTEBANK/quotes-2018.json.bz2'\n",
    "path_to_out = '.../temp/QUOTEBANK/people_quotes-2018.json.bz2'\n",
    "\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            qids = instance['qids'] # extracting list of qids\n",
    "            gender = []\n",
    "            dob = []\n",
    "            for qid in qids:\n",
    "                for qid_wiki in dict_wikidata_people.items() : \n",
    "                    if qid == qid_wiki[0] : \n",
    "                        gender = q[1]['genderlabel']\n",
    "                        dob = q[1]['dob_std']\n",
    "                        gender.append(gender)\n",
    "                        dob.append(dob)\n",
    "                    else :\n",
    "                        gender.append('None')\n",
    "                        dob.append('None')\n",
    "            instance['gender'] = gender # updating the sample with gender\n",
    "            instance['dob'] = dob # updating the sample with date of bith\n",
    "            d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) # writing in the new file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a90f4",
   "metadata": {},
   "source": [
    "### Filtering out data\n",
    "We filter it for the first question by choosing quotes that contains the word \"movie\", \"cinema\" or \"film\". We also choose the quotes that contains these words in their urls. \n",
    "We tried looking for film names in the quotes but it gave a lot of false positiv (a lot of films are named with common english words). Therefore for the first question we used only this filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db42c544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_file = '../generated/QUOTEBANK/people_quotes-2018.json.bz2'\n",
    "path_to_out = '.../temp/QUOTEBANK/movie_quotes-2018.json.bz2'\n",
    "\n",
    "with bz2.open(file, 'rb') as in_file:\n",
    "    with bz2.open(path_to_out, 'wb') as out_file:\n",
    "        for instance in in_file:\n",
    "            instance = json.loads(instance)\n",
    "            quote = instance['quotation']\n",
    "            urls = instance['urls']\n",
    "            if 'cinema' in quote or 'film' in quote or 'movie' in quote:\n",
    "                d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))\n",
    "            elif:\n",
    "                for url in urls:\n",
    "                    if 'cinema' in url or 'film' in url or 'movie' in url:\n",
    "                        d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb1ae9c",
   "metadata": {},
   "source": [
    "### Open complete quotbank 2018 and search movies in quotes\n",
    "For the second question, we need to know which quotes speak about wich movie. For that we took the empath library and customized it to match our need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65d072a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-12T17:46:06.997831Z",
     "iopub.status.busy": "2021-12-12T17:46:06.997381Z",
     "iopub.status.idle": "2021-12-12T17:46:07.460161Z",
     "shell.execute_reply": "2021-12-12T17:46:07.459095Z",
     "shell.execute_reply.started": "2021-12-12T17:46:06.997786Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lexicon initialization\n",
    "lexicon = Empath('ressources/empath_cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c1197-108b-4099-bef9-31386871e86a",
   "metadata": {},
   "source": [
    "We use a list that comes from prepro_IMDb.ipynb. It contains the english names of all films. With this list we create an empath category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f6c40b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T13:55:34.210988Z",
     "iopub.status.busy": "2021-12-13T13:55:34.209851Z",
     "iopub.status.idle": "2021-12-13T13:55:34.619402Z",
     "shell.execute_reply": "2021-12-13T13:55:34.617444Z",
     "shell.execute_reply.started": "2021-12-13T13:55:34.210913Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525377\n"
     ]
    }
   ],
   "source": [
    "#open the pickle list with names of the movies\n",
    "with open('../generated/IMDb/film_name_list.pickle', 'rb') as f: \n",
    "     list_imdb_san = pickle.loads(f.read())\n",
    "list_imdb_san = list(set(list_imdb_san))\n",
    "print(len(list_imdb_san))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "672abcf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-12T16:49:23.784998Z",
     "iopub.status.busy": "2021-12-12T16:49:23.784285Z",
     "iopub.status.idle": "2021-12-12T16:49:24.722315Z",
     "shell.execute_reply": "2021-12-12T16:49:24.720823Z",
     "shell.execute_reply.started": "2021-12-12T16:49:23.784944Z"
    }
   },
   "outputs": [],
   "source": [
    "#create a categorie with all movies\n",
    "lexicon.create_category('ADA_film_name',list_imdb_san)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c58d5b-411d-4e25-afe1-54a5f04163a3",
   "metadata": {},
   "source": [
    "We filter the dataset to keep only the quotes conatining a film name. We write a line per film, for example if a quote contains the names of two films, we write 2 lines for it. The purpose it to make the groupby in Question2 easier. We also add two columns: the id of the film and the name of the film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678caddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reseach movies in quote and add a column 'film' in the dataset\n",
    "path_to_file = '../generated/QUOTEBANK/people_quotes-2018.json.bz2'\n",
    "path_to_out = '../temp/QUOTEBANK/moviefiltered_10tk_quotes-2018.json.bz2'\n",
    "\n",
    "lexicon = Empath('ressources/empath_cat')\n",
    "\n",
    "# dictionnary coming from IMDb.ipynb maping tconst id to film t\n",
    "with open('../generated/IMDb/tconst_title_dict.pickle', 'rb') as f:\n",
    "    tconst_title_dict = pickle.loads(f.read())\n",
    "# dictionary maping film name to id\n",
    "invert_tconst_dict = {v: k for k, v in tconst_title_dict.items()}\n",
    "\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        n = 0\n",
    "        for instance in s_file:\n",
    "            n += 1\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            if instance['quotation'] != 'None':\n",
    "                features = lexicon.analyze(instance['quotation'],['ADA_film_name'],tokenizer=10,verbose=True,debug = True)\n",
    "                if features['count']['ADA_film_name']>0 :\n",
    "                    for film in features['match']:\n",
    "                        result = instance.copy()\n",
    "                        result['film'] = film\n",
    "                        try:\n",
    "                            result['id_film'] = invert_tconst_dict[film.replace(\"_\", \" \")]\n",
    "                        except KeyError:\n",
    "                            result['id_film'] = 'Unknown'\n",
    "                        result.pop('probas', None)\n",
    "                        result.pop('urls', None)\n",
    "                        result.pop('qids', None)\n",
    "                        d_file.write((json.dumps(result)+'\\n').encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038eceb2-0ae5-4184-a3b0-62032f61e803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ec65bdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-15T11:31:29.835118Z",
     "iopub.status.busy": "2021-12-15T11:31:29.834650Z",
     "iopub.status.idle": "2021-12-15T12:34:51.589486Z",
     "shell.execute_reply": "2021-12-15T12:34:51.588618Z",
     "shell.execute_reply.started": "2021-12-15T11:31:29.835078Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c271b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
