{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b07368c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from ressources import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dca3f7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T12:57:37.517395Z",
     "iopub.status.busy": "2021-11-12T12:57:37.516729Z",
     "iopub.status.idle": "2021-11-12T12:57:37.525448Z",
     "shell.execute_reply": "2021-11-12T12:57:37.522145Z",
     "shell.execute_reply.started": "2021-11-12T12:57:37.517349Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAW_DATA_FOLDER = config.RAW_DATA_FOLDER\n",
    "GENERATED_DATA_FOLDER = config.GENERATED_DATA_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da8fa8",
   "metadata": {},
   "source": [
    "## QUOTEBANK Dataset\n",
    "Exploring and filtering of quotbank Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df0d3430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T12:57:58.833110Z",
     "iopub.status.busy": "2021-11-12T12:57:58.832285Z",
     "iopub.status.idle": "2021-11-12T12:57:58.846809Z",
     "shell.execute_reply": "2021-11-12T12:57:58.845665Z",
     "shell.execute_reply.started": "2021-11-12T12:57:58.833057Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "QUOTEBANK_FOLDER = RAW_DATA_FOLDER / \"QUOTEBANK\"\n",
    "file_list = list(QUOTEBANK_FOLDER.glob('*.json.bz2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd79703",
   "metadata": {},
   "source": [
    "### List of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5f8bee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T12:58:22.856057Z",
     "iopub.status.busy": "2021-11-12T12:58:22.855717Z",
     "iopub.status.idle": "2021-11-12T12:58:22.910282Z",
     "shell.execute_reply": "2021-11-12T12:58:22.908739Z",
     "shell.execute_reply.started": "2021-11-12T12:58:22.856030Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns quotebank:\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "with pd.read_json(file_list[0], lines=True, compression='bz2', chunksize=1) as df_reader:\n",
    "    for chunk in df_reader:\n",
    "        df_quotebank = chunk\n",
    "        break\n",
    "# column list for Quotebank dataset\n",
    "print(f\"\\nColumns quotebank:\\n{df_quotebank.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed31fde",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da0f718",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T12:58:42.913304Z",
     "iopub.status.busy": "2021-11-12T12:58:42.912758Z",
     "iopub.status.idle": "2021-11-12T12:58:42.997511Z",
     "shell.execute_reply": "2021-11-12T12:58:42.995848Z",
     "shell.execute_reply.started": "2021-11-12T12:58:42.913256Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample quotebank:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-06-30-000005</td>\n",
       "      <td>... a minimum of 5.25 trillion (plastic) parti...</td>\n",
       "      <td>Marcus Eriksen</td>\n",
       "      <td>[Q55997400]</td>\n",
       "      <td>2018-06-30 07:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>[[Marcus Eriksen, 0.6814], [None, 0.3186]]</td>\n",
       "      <td>[http://www.santacruzsentinel.com/environment-...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2018-06-30-000005  ... a minimum of 5.25 trillion (plastic) parti...   \n",
       "\n",
       "          speaker         qids                date  numOccurrences  \\\n",
       "0  Marcus Eriksen  [Q55997400] 2018-06-30 07:00:00               3   \n",
       "\n",
       "                                       probas  \\\n",
       "0  [[Marcus Eriksen, 0.6814], [None, 0.3186]]   \n",
       "\n",
       "                                                urls phase  \n",
       "0  [http://www.santacruzsentinel.com/environment-...     E  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample for Quotebank dataset\n",
    "print(\"\\nSample quotebank:\\n\")\n",
    "display(df_quotebank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e1c4b",
   "metadata": {},
   "source": [
    "### Number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd8febf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of rows for quotebank dataset\n",
    "chunksize = 10000\n",
    "for file in file_list:\n",
    "    n = 0\n",
    "    print(f\"Processing file {file.name}\")\n",
    "    with pd.read_json(file, lines=True, compression='bz2', chunksize=chunksize) as df_reader:\n",
    "        for chunk in df_reader:\n",
    "            n += len(chunk)\n",
    "            print(n, end = \"\\r\")\n",
    "    \n",
    "    print(f\"{n} rows in {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b54426",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-10T09:42:24.070186Z",
     "iopub.status.busy": "2021-11-10T09:42:24.067422Z",
     "iopub.status.idle": "2021-11-10T10:39:35.926430Z",
     "shell.execute_reply": "2021-11-10T10:39:35.925677Z",
     "shell.execute_reply.started": "2021-11-10T09:42:24.070126Z"
    },
    "tags": []
   },
   "source": [
    "Output:\n",
    "\n",
    "Processing file quotes-2015.json.bz2</br>\n",
    "20874338 rows in quotes-2015.json.bz2</br>\n",
    "Processing file quotes-2016.json.bz2</br>\n",
    "13862129 rows in quotes-2016.json.bz2</br>\n",
    "Processing file quotes-2017.json.bz2</br>\n",
    "26611588 rows in quotes-2017.json.bz2</br>\n",
    "Processing file quotes-2018.json.bz2</br>\n",
    "27228451 rows in quotes-2018.json.bz2</br>\n",
    "Processing file quotes-2019.json.bz2</br>\n",
    "21763302 rows in quotes-2019.json.bz2</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7df6788",
   "metadata": {},
   "source": [
    "### Filtering out data\n",
    "We decided to keep only the data that contains some keywords in the quote itself or in the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80dce18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in file_list:\n",
    "    path_to_out = GENERATED_DATA_FOLDER / \"QUOTEBANK\"\n",
    "    path_to_out = path_to_out / f\"{file.name.split('.', 1)[0]}-cinema.{file.name.split('.', 1)[1]}\"\n",
    "    with bz2.open(file, 'rb') as in_file:\n",
    "        with bz2.open(path_to_out, 'wb') as out_file:\n",
    "            for instance in in_file:\n",
    "                instance = json.loads(instance)\n",
    "                quote = instance['quotation']\n",
    "                urls = instance['urls']\n",
    "                if 'cinema' in quote or 'film' in quote or 'movie' in quote:\n",
    "                    d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))\n",
    "                elif:\n",
    "                    for url in urls:\n",
    "                        if 'cinema' in url or 'film' in url or 'movie' in url:\n",
    "                            d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f261bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a column 'year' with the year of the citation. Usefull to calculate the age of the speaker.\n",
    "path_to_file = 'generated/QUOTEBANK/sample/quotes-2020_1pct.json.bz2'\n",
    "path_to_out = Path('../generated/QUOTEBANK/quotes-2020_1pct_filtered.json.bz2')\n",
    "\n",
    "df_reader = pd.read_json(path_to_file, lines=True, compression='bz2', chunksize=500000)\n",
    "\n",
    "with open(path_to_out, 'wb') as d_file:\n",
    "    for chunk in df_reader:\n",
    "        chunk['date'] = chunk['date'].astype(str)\n",
    "        chunk['year'] = chunk['date'].str[:4]\n",
    "        chunk.to_json(d_file, orient = 'records', compression='bz2', lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdae98c-4444-4248-9525-38eb898b976f",
   "metadata": {},
   "source": [
    "## WIKIDATA Dataset\n",
    "Exploring and filtering of Wikidata Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfa85e9e-0b0c-4936-9005-ef7f40f2862a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T16:44:50.732708Z",
     "iopub.status.busy": "2021-11-12T16:44:50.732060Z",
     "iopub.status.idle": "2021-11-12T16:45:32.679613Z",
     "shell.execute_reply": "2021-11-12T16:45:32.678260Z",
     "shell.execute_reply.started": "2021-11-12T16:44:50.732663Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>aliases</th>\n",
       "      <th>label</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6306473</th>\n",
       "      <td>Q65286219</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[+1954-01-01T00:00:00Z]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2827006</th>\n",
       "      <td>Q56724375</td>\n",
       "      <td>None</td>\n",
       "      <td>Pierre Honoré</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8275800</th>\n",
       "      <td>Q20015257</td>\n",
       "      <td>None</td>\n",
       "      <td>Samuel Giménez</td>\n",
       "      <td>[+1991-02-10T00:00:00Z]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7021994</th>\n",
       "      <td>Q11271525</td>\n",
       "      <td>None</td>\n",
       "      <td>Takayuki Daimon</td>\n",
       "      <td>[+1962-01-01T00:00:00Z]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4874535</th>\n",
       "      <td>Q19808393</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q6581097]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2774175</th>\n",
       "      <td>Q47453933</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[+1916-00-00T00:00:00Z]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8048046</th>\n",
       "      <td>Q4519700</td>\n",
       "      <td>None</td>\n",
       "      <td>Vladimir Shalimov</td>\n",
       "      <td>[+1908-08-08T00:00:00Z]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141070</th>\n",
       "      <td>Q5239154</td>\n",
       "      <td>None</td>\n",
       "      <td>David Rocker</td>\n",
       "      <td>[+1943-01-01T00:00:00Z]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200709</th>\n",
       "      <td>Q95156079</td>\n",
       "      <td>None</td>\n",
       "      <td>Vladimír Doležal</td>\n",
       "      <td>[+1958-07-09T00:00:00Z]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6541052</th>\n",
       "      <td>Q91788272</td>\n",
       "      <td>None</td>\n",
       "      <td>Carlo Fusaro</td>\n",
       "      <td>[+1950-11-19T00:00:00Z]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id aliases              label            date_of_birth  \\\n",
       "6306473  Q65286219    None               None  [+1954-01-01T00:00:00Z]   \n",
       "2827006  Q56724375    None      Pierre Honoré                     None   \n",
       "8275800  Q20015257    None     Samuel Giménez  [+1991-02-10T00:00:00Z]   \n",
       "7021994  Q11271525    None    Takayuki Daimon  [+1962-01-01T00:00:00Z]   \n",
       "4874535  Q19808393    None               None                     None   \n",
       "2774175  Q47453933    None               None  [+1916-00-00T00:00:00Z]   \n",
       "8048046   Q4519700    None  Vladimir Shalimov  [+1908-08-08T00:00:00Z]   \n",
       "141070    Q5239154    None       David Rocker  [+1943-01-01T00:00:00Z]   \n",
       "3200709  Q95156079    None   Vladimír Doležal  [+1958-07-09T00:00:00Z]   \n",
       "6541052  Q91788272    None       Carlo Fusaro  [+1950-11-19T00:00:00Z]   \n",
       "\n",
       "             gender  \n",
       "6306473  [Q6581097]  \n",
       "2827006        None  \n",
       "8275800  [Q6581097]  \n",
       "7021994  [Q6581097]  \n",
       "4874535  [Q6581097]  \n",
       "2774175  [Q6581097]  \n",
       "8048046  [Q6581097]  \n",
       "141070   [Q6581097]  \n",
       "3200709  [Q6581097]  \n",
       "6541052  [Q6581097]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(RAW_DATA_FOLDER / 'WIKIDATA')\n",
    "wiki = df[['id','aliases', 'label','date_of_birth','gender']]\n",
    "wiki.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d31bd88-18bc-4f51-a8ca-8d2257ea3012",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T16:35:00.485876Z",
     "iopub.status.busy": "2021-11-12T16:35:00.485461Z",
     "iopub.status.idle": "2021-11-12T16:35:00.502640Z",
     "shell.execute_reply": "2021-11-12T16:35:00.500720Z",
     "shell.execute_reply.started": "2021-11-12T16:35:00.485828Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def try_join(cell):\n",
    "    try:\n",
    "        return ''.join(map(str, cell))\n",
    "    except TypeError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc421b-40dc-4e5f-b428-328d38980126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki['genderlabel'] = [try_join(cell) for cell in wiki['gender']]\n",
    "wiki['genderlabel'].replace(\"Q6581097\", 0, inplace = True)\n",
    "wiki['genderlabel'].replace(\"Q6581072\", 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1de83-4206-4605-a9c7-9ff174ca1c8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki['date_of_birth'] = [try_join(cell) for cell in wiki['date_of_birth']]\n",
    "wiki['date_of_birth_formatted'] = pd.to_datetime(wiki['date_of_birth'], format = '+%Y-%m-%dT%H:%M:%SZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8235a",
   "metadata": {},
   "source": [
    "## The Internet Movie Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6fc168",
   "metadata": {},
   "source": [
    "### The datasets of the IMDb \n",
    "\n",
    "The Internet Movie Database is an open source database that contains informations regarding movies, TV series, TV movies and even video games. This database which is hosted on a website, is used to rates and simply record characteristics of each features present. The dataset can be found on the IMDB.com website, and it is described as follows :\n",
    "\n",
    "Each dataset is contained in a gzipped, tab-separated-values (TSV) formatted file in the UTF-8 character set. The first line in each file contains headers that describe what is in each column. A ‘\\N’ is used to denote that a particular field is missing or null for that title/name. The available datasets are as follows:\n",
    "\n",
    "title.akas.tsv.gz - Contains the following information for titles:\n",
    "\n",
    "- titleId (string) - a tconst, an alphanumeric unique identifier of the title\n",
    "- ordering (integer) – a number to uniquely identify rows for a given titleId\n",
    "- title (string) – the localized title\n",
    "- region (string) - the region for this version of the title\n",
    "- language (string) - the language of the title\n",
    "- types (array) - Enumerated set of attributes for this alternative title. One or more of the following: \"alternative\", \"dvd\", \"festival\", \"tv\", \"video\", \"working\", \"original\", \"imdbDisplay\". New values may be added in the future without warning\n",
    "- attributes (array) - Additional terms to describe this alternative title, not enumerated\n",
    "- isOriginalTitle (boolean) – 0: not original title; 1: original title\n",
    "\n",
    "title.basics.tsv.gz - Contains the following information for titles:\n",
    "\n",
    "- tconst (string) - alphanumeric unique identifier of the title\n",
    "- titleType (string) – the type/format of the title (e.g. movie, short, tvseries, tvepisode, video, etc)\n",
    "- primaryTitle (string) – the more popular title / the title used by the filmmakers on promotional materials at the point of release\n",
    "- originalTitle (string) - original title, in the original language\n",
    "- isAdult (boolean) - 0: non-adult title; 1: adult title\n",
    "- startYear (YYYY) – represents the release year of a title. In the case of TV Series, it is the series start year\n",
    "- endYear (YYYY) – TV Series end year. ‘\\N’ for all other title types\n",
    "- runtimeMinutes – primary runtime of the title, in minutes\n",
    "- genres (string array) – includes up to three genres associated with the title\n",
    "\n",
    "title.crew.tsv.gz – Contains the director and writer information for all the titles in IMDb. Fields include:\n",
    "\n",
    "- tconst (string) - alphanumeric unique identifier of the title\n",
    "- directors (array of nconsts) - director(s) of the given title\n",
    "- writers (array of nconsts) – writer(s) of the given title\n",
    "- title.episode.tsv.gz – Contains the tv episode information. Fields include:\n",
    "- tconst (string) - alphanumeric identifier of episode\n",
    "- parentTconst (string) - alphanumeric identifier of the parent TV Series\n",
    "- seasonNumber (integer) – season number the episode belongs to\n",
    "- episodeNumber (integer) – episode number of the tconst in the TV series\n",
    "\n",
    "title.principals.tsv.gz – Contains the principal cast/crew for titles\n",
    "\n",
    "- tconst (string) - alphanumeric unique identifier of the title\n",
    "- ordering (integer) – a number to uniquely identify rows for a given titleId\n",
    "- nconst (string) - alphanumeric unique identifier of the name/person\n",
    "- category (string) - the category of job that person was in\n",
    "- job (string) - the specific job title if applicable, else '\\N'\n",
    "- characters (string) - the name of the character played if applicable, else '\\N'\n",
    "\n",
    "title.ratings.tsv.gz – Contains the IMDb rating and votes information for titles\n",
    "\n",
    "- tconst (string) - alphanumeric unique identifier of the title\n",
    "- averageRating – weighted average of all the individual user ratings\n",
    "- numVotes - number of votes the title has received\n",
    "\n",
    "name.basics.tsv.gz – Contains the following information for names:\n",
    "\n",
    "- nconst (string) - alphanumeric unique identifier of the name/person\n",
    "- primaryName (string)– name by which the person is most often credited\n",
    "- birthYear – in YYYY format\n",
    "- deathYear – in YYYY format if applicable, else '\\N'\n",
    "- primaryProfession (array of strings)– the top-3 professions of the person\n",
    "- knownForTitles (array of tconsts) – titles the person is known for\n",
    "\n",
    "source : https://www.imdb.com/interfaces/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1166c3",
   "metadata": {},
   "source": [
    "### Goal to achieve \n",
    "In this IMDb pre-processing notebook, we want to import the datasets that were defined as being of interest (not all were taken as title.akas.tsv.gz was left behind - sorry title.akas.tsv.gz) and treat them in order to obtain a main dataset, with rows and columns of interest regarding our project. As described before, the database doesn't concern only movies, so the main dataset will have to be filtered as we will not consider series - for example. \n",
    "\n",
    "Datasets were merged using the different ids (tconst and nconst) that link tables among them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f349b",
   "metadata": {},
   "source": [
    "### Importing datasets\n",
    "In this section, we are importing the datasets of interest as dataframes. Datasets are compressed into .gz archives and None values where directly replaced when importing them, in order to facilitate the processing.\n",
    "\n",
    "Nota Bene : as the datasets are of size around 300MB each, it is not possible to push them in the git repository (limited to 100MB). Datasets where opened locally and so the paths below can't be run.\n",
    "\n",
    "The link to download the datasets is : https://datasets.imdbws.com/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f8e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "principals = './data/title.principals.tsv.gz'\n",
    "names = './data/name.basics.tsv.gz'\n",
    "akas = './data/title.akas.tsv.gz'\n",
    "titles = './data/title.basics.tsv.gz'\n",
    "crew = './data/title.crew.tsv.gz'\n",
    "ratings = './data/title.ratings.tsv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34c0a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = pd.read_csv(names, \n",
    "                       compression = \"infer\",\n",
    "                       sep = '\\t',\n",
    "                       na_values = '\\\\N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a4f9b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_principals = pd.read_csv(principals, \n",
    "                            compression = \"infer\",\n",
    "                            sep = '\\t',\n",
    "                            na_values = '\\\\N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50dc493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\celin\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df_titles = pd.read_csv(titles, \n",
    "            compression = \"infer\",\n",
    "            sep = '\\t',\n",
    "            na_values = '\\\\N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9259e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crew = pd.read_csv(crew, \n",
    "            compression=\"infer\",\n",
    "            sep = '\\t',\n",
    "            na_values = '\\\\N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7df8bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv(ratings, \n",
    "            compression=\"infer\",\n",
    "            sep = '\\t',\n",
    "            na_values = '\\\\N')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f588e0e4",
   "metadata": {},
   "source": [
    "### Processing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d14cce",
   "metadata": {},
   "source": [
    "1) Starting with df_name : \n",
    "Here, it is only needed to drop columns that would not be in use for the project, which are the birth year and death year of each people present in the IMDb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8016de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names.drop(columns = ['birthYear', 'deathYear'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721367cc",
   "metadata": {},
   "source": [
    "2) Then with df_principals : this dataframe needs more processing as we want to transform values of some column into a dict. Indeed, as there are multiple actors, actresses or crew members for each movies in the database, the dataframe will be separate into two parts (respectively actors/actresses and crew members). Each columsn of each rows of the separated dataframes are then aggregated into one dictionary - one dictionary containing, as example, the id/ the name/ the category of the concerned person. Finally, people are merged together into a list of dictionaries, to obtain a final dataframe which has one row per movie containing all crew members, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0886c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of the dataframe column 'nconst'\n",
    "df_principals['name'] = df_principals['nconst'].copy()\n",
    "\n",
    "#mapping nconst values of the dataframe with names that are present in the df_names dataframe (based on the id nconst)\n",
    "df_principals['name'] = df_principals['name'].map(df_names.set_index('nconst')['primaryName'])\n",
    "df_principals = df_principals [['tconst', 'ordering', 'nconst', 'name', 'category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd178349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-520412db787b>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_principals_actors['actor/actress'] = actors_dictionary.values()\n"
     ]
    }
   ],
   "source": [
    "# separating dataset by keeping rows where category = actors or actresses \n",
    "actors = ['actor', 'actress']\n",
    "df_principals_actors = df_principals[df_principals.category.isin(actors)]\n",
    "\n",
    "# creating a new dataframe to transform columns into a dictionary\n",
    "# set index nconst and drop tconst & ordering\n",
    "df_principals_actors_tmp = df_principals_actors.copy()\n",
    "df_principals_actors_tmp = df_principals_actors_tmp.set_index(['tconst', 'ordering'])\n",
    "actors_dictionary = df_principals_actors_tmp.to_dict('index')\n",
    "\n",
    "# replacing with dict values \n",
    "df_principals_actors['actor/actress'] = actors_dictionary.values()\n",
    "\n",
    "# as there are multiple rows for each movie (as there are multiple crew member), aggregating rows by movie's id and so creating a list of dict\n",
    "# on the column actor/actress\n",
    "df_principals_actors = df_principals_actors.groupby(['tconst']).agg(lambda x: tuple(x)).applymap(list).reset_index()\n",
    "df_principals_actors = df_principals_actors.drop(columns = ['category', 'ordering', 'nconst', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a99e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating dataset by keeping the rows concerning the crew and same as before\n",
    "actors = ['actor', 'actress', 'self']\n",
    "df_principals_crew = df_principals[~df_principals['category'].isin(actors)]\n",
    "\n",
    "# creating a new dataframe to transform columns into a dictionary\n",
    "# set index nconst and drop tconst & ordering\n",
    "df_principals_crew_tmp = df_principals_crew.copy()\n",
    "df_principals_crew_tmp = df_principals_crew_tmp.set_index(['tconst', 'ordering'])\n",
    "crew_dictionary = df_principals_crew_tmp.to_dict('index')\n",
    "\n",
    "# replacing with dict values \n",
    "df_principals_crew['crew'] = crew_dictionary.values()\n",
    "\n",
    "# as there are multiple rows for each movie (as there are multiple crew member), aggregating rows by movie's id and so creating a list of dict\n",
    "# on the column actor/actress\n",
    "df_principals_crew= df_principals_crew.groupby(['tconst']).agg(lambda x: tuple(x)).applymap(list).reset_index()\n",
    "df_principals_crew = df_principals_crew.drop(columns = ['category', 'ordering', 'name', 'nconst'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412c7b7",
   "metadata": {},
   "source": [
    "3) Processing the dataframe df_titles, by only dropping some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb29d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titles.drop(columns = ['endYear', 'isAdult', 'primaryTitle'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea8f4b",
   "metadata": {},
   "source": [
    "### Merging the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db232fc7",
   "metadata": {},
   "source": [
    "Here, we are merging all datasets into one main dataset, by simple aggregating columns based on the 'tconst' id, which are ids of movies. But first, rows have to be filtered to keep only the ones which concern movies !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbb50a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging df_crew and df_titles to have a dataframe containing movies and there respective crew\n",
    "merged = pd.merge(df_titles, df_crew, on = 'tconst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80e38365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['short', 'movie', 'tvEpisode', 'tvSeries', 'tvShort', 'tvMovie',\n",
       "       'tvMiniSeries', 'tvSpecial', 'video', 'videoGame', 'tvPilot'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking types considered in the database\n",
    "merged['titleType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dee20df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataframe contains now 593343 rows\n"
     ]
    }
   ],
   "source": [
    "# keeping only rows concerning movies\n",
    "merged.drop(merged.loc[merged['titleType'] != 'movie'].index, inplace=True)\n",
    "\n",
    "print(f'the dataframe contains now {len(merged)} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8040657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing some fancy rearranging\n",
    "merged = merged.drop(columns = ['titleType'])\n",
    "merged = merged.rename(columns = {'startYear' : 'year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "202d6e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.merge(df_principals_actors, on = 'tconst', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f12727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.merge(df_principals_crew, on = 'tconst', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a863ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding ratings from df_ratings (now we have movies and there title, the crew associated and the ratings)\n",
    "merged = merged.merge(df_ratings, on = 'tconst', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3acf7a",
   "metadata": {},
   "source": [
    "Saving into pickle (why not, can be useful) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "250c31fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5462c61927ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmerged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'main_df.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'merged' is not defined"
     ]
    }
   ],
   "source": [
    "merged.to_pickle('main_df.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490e6031",
   "metadata": {},
   "source": [
    "### Saving main dataframe into json\n",
    "Here, we simply save the dataframe in json format. json was chosen of behalf of csv format, to match the quotebank dataset which is also in json and to more easily play with the dicts created on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a719eb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-6d32313a9fc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"IMDb.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0md_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'records'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bz2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_json\u001b[1;34m(self, path_or_buf, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent)\u001b[0m\n\u001b[0;32m   2350\u001b[0m         \u001b[0mindent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindent\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2352\u001b[1;33m         return json.to_json(\n\u001b[0m\u001b[0;32m   2353\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mto_json\u001b[1;34m(path_or_buf, obj, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "with open(\"IMDb.json.bz2\", 'wb') as d_file:\n",
    "    d_file.write(merged.to_json(orient = 'records', compression='bz2', lines = True).encode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee5ce95",
   "metadata": {},
   "source": [
    "### Prepare another dict for further work with IMDb.json\n",
    "Creating a dict that will be used to end the pre-processing of this database. As some columns of the previous main dataframe are still imperfect (directors and writers columns have nconst values instead of names), we're creating a dict that simply link nconst with names. It will be also useful to handle the Wikidata dataset which contains the same ids (nconst) as found in the IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8c372db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-9eb7e8aca6e4>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_nconst['id'] = df_nconst['nconst'].copy()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nconst</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nm0000001</th>\n",
       "      <td>nm0000001</td>\n",
       "      <td>Fred Astaire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nm0000002</th>\n",
       "      <td>nm0000002</td>\n",
       "      <td>Lauren Bacall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nm0000003</th>\n",
       "      <td>nm0000003</td>\n",
       "      <td>Brigitte Bardot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nm0000004</th>\n",
       "      <td>nm0000004</td>\n",
       "      <td>John Belushi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nm0000005</th>\n",
       "      <td>nm0000005</td>\n",
       "      <td>Ingmar Bergman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nm9993714</th>\n",
       "      <td>nm9993714</td>\n",
       "      <td>Romeo del Rosario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nm9993716</th>\n",
       "      <td>nm9993716</td>\n",
       "      <td>Essias Loberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nm9993717</th>\n",
       "      <td>nm9993717</td>\n",
       "      <td>Harikrishnan Rajan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nm9993718</th>\n",
       "      <td>nm9993718</td>\n",
       "      <td>Aayush Nair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nm9993719</th>\n",
       "      <td>nm9993719</td>\n",
       "      <td>Andre Hill</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11357768 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              nconst                name\n",
       "id                                      \n",
       "nm0000001  nm0000001        Fred Astaire\n",
       "nm0000002  nm0000002       Lauren Bacall\n",
       "nm0000003  nm0000003     Brigitte Bardot\n",
       "nm0000004  nm0000004        John Belushi\n",
       "nm0000005  nm0000005      Ingmar Bergman\n",
       "...              ...                 ...\n",
       "nm9993714  nm9993714   Romeo del Rosario\n",
       "nm9993716  nm9993716       Essias Loberg\n",
       "nm9993717  nm9993717  Harikrishnan Rajan\n",
       "nm9993718  nm9993718         Aayush Nair\n",
       "nm9993719  nm9993719          Andre Hill\n",
       "\n",
       "[11357768 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a dict with nconst as keys and names as values\n",
    "df_nconst = df_names[['nconst', 'primaryName']]\n",
    "df_nconst['id'] = df_nconst['nconst'].copy()\n",
    "df_nconst = df_nconst.rename(columns = {'primaryName' : 'name'})\n",
    "df_nconst.set_index('id')\n",
    "\n",
    "nconst_names = df_nconst.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09122620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dict into pickle\n",
    "nconst_names.to_pickle('nconst_names.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0532d55",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-980e2e90d94a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nconst_names.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;31m# a debuggability cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# writing dict to json file\n",
    "with open('nconst_names.json', 'wb') as file:\n",
    "    json.dump(dict, file).encode('utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
