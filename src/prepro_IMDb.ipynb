{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b07368c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T20:21:47.884162Z",
     "iopub.status.busy": "2021-11-12T20:21:47.883565Z",
     "iopub.status.idle": "2021-11-12T20:21:53.086772Z",
     "shell.execute_reply": "2021-11-12T20:21:53.085800Z",
     "shell.execute_reply.started": "2021-11-12T20:21:47.884117Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from ressources import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dca3f7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T20:21:56.475843Z",
     "iopub.status.busy": "2021-11-12T20:21:56.475240Z",
     "iopub.status.idle": "2021-11-12T20:21:56.489057Z",
     "shell.execute_reply": "2021-11-12T20:21:56.485127Z",
     "shell.execute_reply.started": "2021-11-12T20:21:56.475812Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAW_DATA_FOLDER = config.RAW_DATA_FOLDER\n",
    "GENERATED_DATA_FOLDER = config.GENERATED_DATA_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894c0f1-ed71-4dcb-85ed-5ff16a8ce67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitarization(word_list):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    word_list_san = []   \n",
    "    for film in word_list:\n",
    "        doc_film_name = nlp(film)\n",
    "        tokens = [token.text for token in doc_film_name]\n",
    "        result = \" \".join(tokens)\n",
    "        word_list_san.append(result)\n",
    "    return word_list_san"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c0df3a-5fb4-41f3-a309-8fe93ccd4c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "principals = './data/title.principals.tsv.gz'\n",
    "names = './data/name.basics.tsv.gz'\n",
    "akas = './data/title.akas.tsv.gz'\n",
    "titles = './data/title.basics.tsv.gz'\n",
    "crew = './data/title.crew.tsv.gz'\n",
    "ratings = './data/title.ratings.tsv.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8235a",
   "metadata": {},
   "source": [
    "## The Internet Movie Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6fc168",
   "metadata": {},
   "source": [
    "### The datasets of the IMDb \n",
    "\n",
    "The Internet Movie Database is an open source database that contains informations regarding movies, TV series, TV movies and even video games. This database which is hosted on a website, is used to rates and simply record characteristics of each features present. The dataset can be found on the IMDB.com website, and it is described as follows :\n",
    "\n",
    "Each dataset is contained in a gzipped, tab-separated-values (TSV) formatted file in the UTF-8 character set. The first line in each file contains headers that describe what is in each column. A ‘\\N’ is used to denote that a particular field is missing or null for that title/name. The available datasets are as follows:\n",
    "\n",
    "title.akas.tsv.gz - Contains the following information for titles:\n",
    "\n",
    "- titleId (string) - a tconst, an alphanumeric unique identifier of the title\n",
    "- ordering (integer) – a number to uniquely identify rows for a given titleId\n",
    "- title (string) – the localized title\n",
    "- region (string) - the region for this version of the title\n",
    "- language (string) - the language of the title\n",
    "- types (array) - Enumerated set of attributes for this alternative title. One or more of the following: \"alternative\", \"dvd\", \"festival\", \"tv\", \"video\", \"working\", \"original\", \"imdbDisplay\". New values may be added in the future without warning\n",
    "- attributes (array) - Additional terms to describe this alternative title, not enumerated\n",
    "- isOriginalTitle (boolean) – 0: not original title; 1: original title\n",
    "\n",
    "title.basics.tsv.gz - Contains the following information for titles:\n",
    "\n",
    "- tconst (string) - alphanumeric unique identifier of the title\n",
    "- titleType (string) – the type/format of the title (e.g. movie, short, tvseries, tvepisode, video, etc)\n",
    "- primaryTitle (string) – the more popular title / the title used by the filmmakers on promotional materials at the point of release\n",
    "- originalTitle (string) - original title, in the original language\n",
    "- isAdult (boolean) - 0: non-adult title; 1: adult title\n",
    "- startYear (YYYY) – represents the release year of a title. In the case of TV Series, it is the series start year\n",
    "- endYear (YYYY) – TV Series end year. ‘\\N’ for all other title types\n",
    "- runtimeMinutes – primary runtime of the title, in minutes\n",
    "- genres (string array) – includes up to three genres associated with the title\n",
    "\n",
    "title.crew.tsv.gz – Contains the director and writer information for all the titles in IMDb. Fields include:\n",
    "\n",
    "- tconst (string) - alphanumeric unique identifier of the title\n",
    "- directors (array of nconsts) - director(s) of the given title\n",
    "- writers (array of nconsts) – writer(s) of the given title\n",
    "- title.episode.tsv.gz – Contains the tv episode information. Fields include:\n",
    "- tconst (string) - alphanumeric identifier of episode\n",
    "- parentTconst (string) - alphanumeric identifier of the parent TV Series\n",
    "- seasonNumber (integer) – season number the episode belongs to\n",
    "- episodeNumber (integer) – episode number of the tconst in the TV series\n",
    "\n",
    "title.principals.tsv.gz – Contains the principal cast/crew for titles\n",
    "\n",
    "- tconst (string) - alphanumeric unique identifier of the title\n",
    "- ordering (integer) – a number to uniquely identify rows for a given titleId\n",
    "- nconst (string) - alphanumeric unique identifier of the name/person\n",
    "- category (string) - the category of job that person was in\n",
    "- job (string) - the specific job title if applicable, else '\\N'\n",
    "- characters (string) - the name of the character played if applicable, else '\\N'\n",
    "\n",
    "title.ratings.tsv.gz – Contains the IMDb rating and votes information for titles\n",
    "\n",
    "- tconst (string) - alphanumeric unique identifier of the title\n",
    "- averageRating – weighted average of all the individual user ratings\n",
    "- numVotes - number of votes the title has received\n",
    "\n",
    "name.basics.tsv.gz – Contains the following information for names:\n",
    "\n",
    "- nconst (string) - alphanumeric unique identifier of the name/person\n",
    "- primaryName (string)– name by which the person is most often credited\n",
    "- birthYear – in YYYY format\n",
    "- deathYear – in YYYY format if applicable, else '\\N'\n",
    "- primaryProfession (array of strings)– the top-3 professions of the person\n",
    "- knownForTitles (array of tconsts) – titles the person is known for\n",
    "\n",
    "source : https://www.imdb.com/interfaces/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1166c3",
   "metadata": {},
   "source": [
    "### Goal to achieve \n",
    "In this IMDb pre-processing notebook, we want to import the datasets that were defined as being of interest (not all were taken as title.akas.tsv.gz was left behind - sorry title.akas.tsv.gz) and treat them in order to obtain a main dataset, with rows and columns of interest regarding our project. As described before, the database doesn't concern only movies, so the main dataset will have to be filtered as we will not consider series - for example. \n",
    "\n",
    "Datasets were merged using the different ids (tconst and nconst) that link tables among them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f349b",
   "metadata": {},
   "source": [
    "### Importing datasets\n",
    "In this section, we are importing the datasets of interest as dataframes. Datasets are compressed into .gz archives and None values where directly replaced when importing them, in order to facilitate the processing.\n",
    "\n",
    "Nota Bene : as the datasets are of size around 300MB each, it is not possible to push them in the git repository (limited to 100MB). Datasets where opened locally and so the paths below can't be run.\n",
    "\n",
    "The link to download the datasets is : https://datasets.imdbws.com/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f588e0e4",
   "metadata": {},
   "source": [
    "### Processing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ad50ee-6d16-4667-85a6-afa42bb14477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = pd.read_csv(names, \n",
    "                       compression = \"infer\",\n",
    "                       sep = '\\t',\n",
    "                       na_values = '\\\\N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f76742-1dbc-4ad5-903c-5d5a2f5e542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_principals = pd.read_csv(principals, \n",
    "                            compression = \"infer\",\n",
    "                            sep = '\\t',\n",
    "                            na_values = '\\\\N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fda17c-59f4-4b7d-931c-25f2a15bb469",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titles = pd.read_csv(titles, \n",
    "            compression = \"infer\",\n",
    "            sep = '\\t',\n",
    "            na_values = '\\\\N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0449fe2-4504-44b3-bfd9-96636d5bac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crew = pd.read_csv(crew, \n",
    "            compression=\"infer\",\n",
    "            sep = '\\t',\n",
    "            na_values = '\\\\N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e91ff-a646-46e4-8630-576998e51bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv(ratings, \n",
    "            compression=\"infer\",\n",
    "            sep = '\\t',\n",
    "            na_values = '\\\\N')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d14cce",
   "metadata": {},
   "source": [
    "1) Starting with df_name : \n",
    "Here, it is only needed to drop columns that would not be in use for the project, which are the birth year and death year of each people present in the IMDb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names.drop(columns = ['birthYear', 'deathYear'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721367cc",
   "metadata": {},
   "source": [
    "2) Then with df_principals : this dataframe needs more processing as we want to transform values of some column into a dict. Indeed, as there are multiple actors, actresses or crew members for each movies in the database, the dataframe will be separate into two parts (respectively actors/actresses and crew members). Each columsn of each rows of the separated dataframes are then aggregated into one dictionary - one dictionary containing, as example, the id/ the name/ the category of the concerned person. Finally, people are merged together into a list of dictionaries, to obtain a final dataframe which has one row per movie containing all crew members, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0886c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of the dataframe column 'nconst'\n",
    "df_principals['name'] = df_principals['nconst'].copy()\n",
    "\n",
    "#mapping nconst values of the dataframe with names that are present in the df_names dataframe (based on the id nconst)\n",
    "df_principals['name'] = df_principals['name'].map(df_names.set_index('nconst')['primaryName'])\n",
    "df_principals = df_principals [['tconst', 'ordering', 'nconst', 'name', 'category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd178349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating dataset by keeping rows where category = actors or actresses \n",
    "actors = ['actor', 'actress']\n",
    "df_principals_actors = df_principals[df_principals.category.isin(actors)]\n",
    "\n",
    "# creating a new dataframe to transform columns into a dictionary\n",
    "# set index nconst and drop tconst & ordering\n",
    "df_principals_actors_tmp = df_principals_actors.copy()\n",
    "df_principals_actors_tmp = df_principals_actors_tmp.set_index(['tconst', 'ordering'])\n",
    "actors_dictionary = df_principals_actors_tmp.to_dict('index')\n",
    "\n",
    "# replacing with dict values \n",
    "df_principals_actors['actor/actress'] = actors_dictionary.values()\n",
    "\n",
    "# as there are multiple rows for each movie (as there are multiple crew member), aggregating rows by movie's id and so creating a list of dict\n",
    "# on the column actor/actress\n",
    "df_principals_actors = df_principals_actors.groupby(['tconst']).agg(lambda x: tuple(x)).applymap(list).reset_index()\n",
    "df_principals_actors = df_principals_actors.drop(columns = ['category', 'ordering', 'nconst', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a99e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating dataset by keeping the rows concerning the crew and same as before\n",
    "actors = ['actor', 'actress', 'self']\n",
    "df_principals_crew = df_principals[~df_principals['category'].isin(actors)]\n",
    "\n",
    "# creating a new dataframe to transform columns into a dictionary\n",
    "# set index nconst and drop tconst & ordering\n",
    "df_principals_crew_tmp = df_principals_crew.copy()\n",
    "df_principals_crew_tmp = df_principals_crew_tmp.set_index(['tconst', 'ordering'])\n",
    "crew_dictionary = df_principals_crew_tmp.to_dict('index')\n",
    "\n",
    "# replacing with dict values \n",
    "df_principals_crew['crew'] = crew_dictionary.values()\n",
    "\n",
    "# as there are multiple rows for each movie (as there are multiple crew member), aggregating rows by movie's id and so creating a list of dict\n",
    "# on the column actor/actress\n",
    "df_principals_crew= df_principals_crew.groupby(['tconst']).agg(lambda x: tuple(x)).applymap(list).reset_index()\n",
    "df_principals_crew = df_principals_crew.drop(columns = ['category', 'ordering', 'name', 'nconst'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412c7b7",
   "metadata": {},
   "source": [
    "3) Processing the dataframe df_titles, by only dropping some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb29d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titles.drop(columns = ['endYear', 'isAdult', 'primaryTitle'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea8f4b",
   "metadata": {},
   "source": [
    "### Merging the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db232fc7",
   "metadata": {},
   "source": [
    "Here, we are merging all datasets into one main dataset, by simple aggregating columns based on the 'tconst' id, which are ids of movies. But first, rows have to be filtered to keep only the ones which concern movies !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb50a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging df_crew and df_titles to have a dataframe containing movies and there respective crew\n",
    "merged = pd.merge(df_titles, df_crew, on = 'tconst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e38365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking types considered in the database\n",
    "merged['titleType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee20df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only rows concerning movies\n",
    "merged.drop(merged.loc[merged['titleType'] != 'movie'].index, inplace=True)\n",
    "\n",
    "print(f'the dataframe contains now {len(merged)} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab11aba-7567-4894-8691-6aeb08537a96",
   "metadata": {},
   "source": [
    "The dataset contains ~500k movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing some fancy rearranging\n",
    "merged = merged.drop(columns = ['titleType'])\n",
    "merged = merged.rename(columns = {'startYear' : 'year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d6e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.merge(df_principals_actors, on = 'tconst', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.merge(df_principals_crew, on = 'tconst', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a863ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding ratings from df_ratings (now we have movies and there title, the crew associated and the ratings)\n",
    "merged = merged.merge(df_ratings, on = 'tconst', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3acf7a",
   "metadata": {},
   "source": [
    "Saving into pickle (why not, can be useful) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_pickle('main_df.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490e6031",
   "metadata": {},
   "source": [
    "### Saving main dataframe into json\n",
    "Here, we simply save the dataframe in json format. json was chosen of behalf of csv format, to match the quotebank dataset which is also in json and to more easily play with the dicts created on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a719eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open(\"IMDb.json.bz2\", 'wb') as d_file:\n",
    "    d_file.write(merged.to_json(orient = 'records', lines = True).encode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee5ce95",
   "metadata": {},
   "source": [
    "### Prepare another dict for further work with IMDb.json\n",
    "Creating a dict that will be used to end the pre-processing of this database. As some columns of the previous main dataframe are still imperfect (directors and writers columns have nconst values instead of names), we're creating a dict that simply link nconst with names. It will be also useful to handle the Wikidata dataset which contains the same ids (nconst) as found in the IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c372db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dict with nconst as keys and names as values\n",
    "df_nconst = df_names[['nconst', 'primaryName']]\n",
    "df_nconst['id'] = df_nconst['nconst'].copy()\n",
    "df_nconst = df_nconst.rename(columns = {'primaryName' : 'name'})\n",
    "df_nconst.set_index('id')\n",
    "\n",
    "nconst_names = df_nconst.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09122620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dict into pickle\n",
    "nconst_names.to_pickle('nconst_names.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0532d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dict into pickle for futur use\n",
    "import pickle\n",
    "\n",
    "with open('nconst_names.pickle', 'wb') as handle:\n",
    "    pickle.dump(nconst_names, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f78c8d-1117-4584-baf8-dd44278927f6",
   "metadata": {},
   "source": [
    "#### Saving lists to pickel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ce0fbf-da92-4b14-bde1-f8053fcdcc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "film_name = list(merged['originalTitle'])\n",
    "with open('generated/film_name_list.pickle', 'wb') as f: \n",
    "    pickle.dump(film_name, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53064dea-ddde-4dca-934e-e4b92eb61e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_name_list = []\n",
    "for index, row in merged.iterrows():\n",
    "    if row['actor/actress'] :\n",
    "        for e in row['actor/actress']:\n",
    "            actor_name = e['name']\n",
    "            person_name_list.append(actor_name)\n",
    "    if row['crew'] :        \n",
    "        for t in row['crew']:\n",
    "            crew_name = t['name']\n",
    "            person_name_list.append(crew_name)\n",
    "\n",
    "#drop duplicates\n",
    "person_name_list = list(set(person_name_list))\n",
    "with open('generated/person_name_list.pickle', 'wb') as f: \n",
    "    pickle.dump(person_name_list, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
